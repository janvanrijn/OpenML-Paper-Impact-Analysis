{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OpenML Impact Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pandas --quiet\n",
        "!pip install matplotlib --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "519xCC91_sMH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz5J5Z7bVhhv"
      },
      "source": [
        "## Data cleaning and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "id": "uAzne5nHARM6",
        "outputId": "8bb8de58-2675-41a3-95b3-42b434181a0e"
      },
      "outputs": [],
      "source": [
        "path1 = '../data/collected_papers.csv' # Original list of collected papers\n",
        "path2 = \"../data/Final_survey_data.csv\" # Survey Results\n",
        "\n",
        "df = pd.read_csv(path2)\n",
        "\n",
        "print(\"Total no of reviews recieved: \", len(df))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "id": "IYZ9Bi1qEWqk",
        "outputId": "25f8705e-2d6e-4d8f-9d16-2ece26af0fa5"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(df.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhQQfvvt9kH8"
      },
      "source": [
        "### Sanity Check:\n",
        "1. Remove duplicates\n",
        "2. Match paper ID and paper title from original list\n",
        "3. Remove papers with wrong year (<2014)\n",
        "4. Remove empty rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4xo6Knlovwe",
        "outputId": "c5284c55-1546-4e12-a756-ab280a6ab0d4"
      },
      "outputs": [],
      "source": [
        "Total_papers = pd.read_csv(path1) # Original list of scraped paper\n",
        "print(\"Total scraped papers: \", len(Total_papers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4s0As6Lq9jp3"
      },
      "outputs": [],
      "source": [
        "assigned_papers_dict = Total_papers[[\"Paper ID\", \"Title\"]].set_index('Paper ID').to_dict()\n",
        "assigned_papers_dict = assigned_papers_dict[\"Title\"]\n",
        "assigned_papers_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sheet1 = Total_papers[[\"Paper ID\", \"openml-suites-2021\", \"openml-python-2021\", \"openml-2014\", \"openml-r-2017\"]]\n",
        "sheet1.rename(columns={\"Paper ID\": \"Paper ID (from shared sheet)\"}, inplace=True)\n",
        "df = df.merge(sheet1, on=\"Paper ID (from shared sheet)\", how=\"left\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ayYUeZPFz3Dd",
        "outputId": "6adb4c42-342a-4bc0-ce93-3f446e172c5a"
      },
      "outputs": [],
      "source": [
        "# Step 1: Remove Dublicates\n",
        "\n",
        "# Duplicate entries\n",
        "duplicate_rows = df[df.duplicated(subset=[\"Paper ID (from shared sheet)\"], keep=False)]\n",
        "\n",
        "print(len(duplicate_rows))\n",
        "duplicate_rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W3tKQM5f0AGk",
        "outputId": "589cc409-3710-4906-dd04-c37dc651a211"
      },
      "outputs": [],
      "source": [
        "duplicates = df[df.duplicated(subset=[\"Paper ID (from shared sheet)\"], keep=False)]\n",
        "# Save duplicates to a separate DataFrame before removal\n",
        "duplicates_removed = df[df.duplicated(subset=[\"Paper ID (from shared sheet)\"], keep=\"first\")]\n",
        "\n",
        "df = df.drop_duplicates(subset=[\"Paper ID (from shared sheet)\"], keep=\"first\")\n",
        "\n",
        "print(len(duplicates_removed))\n",
        "duplicates_removed\n",
        "# ToDo - better strategy to remove duplicates - ask reviewers to correct them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88YrvITVAKTO",
        "outputId": "10eaec5c-2d00-4dfa-9091-5cdb4f518775"
      },
      "outputs": [],
      "source": [
        "print(\"No of reviews after dublicate removal: \", len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "collapsed": true,
        "id": "YWfRQuIsSl_l",
        "outputId": "05b529fa-f243-4e94-eeb1-37127f146215"
      },
      "outputs": [],
      "source": [
        "# Step 2: Remove mismatches\n",
        "\n",
        "mismatches = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    try:\n",
        "        paper_id = row[\"Paper ID (from shared sheet)\"]\n",
        "        paper_title = row[\"Paper Title\"].strip().lower()\n",
        "        assigned_title = assigned_papers_dict.get(paper_id).strip().lower()\n",
        "\n",
        "        if paper_title not in assigned_title:\n",
        "            mismatches.append({\n",
        "                \"Paper ID\": paper_id,\n",
        "                \"Given Title\": row[\"Paper Title\"],\n",
        "                \"Expected Title\": assigned_papers_dict.get(paper_id, \"Not Found\")\n",
        "            })\n",
        "\n",
        "    except Exception as e:\n",
        "        mismatches.append({\n",
        "            \"Paper ID\": row[\"Paper ID (from shared sheet)\"],\n",
        "            \"Error\": str(e),\n",
        "            \"Given Title\": row.get(\"Paper Title\", \"Not Found\")\n",
        "        })\n",
        "        continue\n",
        "\n",
        "mismatches_df = pd.DataFrame(mismatches)\n",
        "mismatches_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "VB7cPdcY2EKD",
        "outputId": "f09f1e8a-6de2-4adc-fb0a-ebc3834526b5"
      },
      "outputs": [],
      "source": [
        "# Mismatched paper ID and title - 901, 240, 881, 1467, 1472, 108 - to be removed\n",
        "paper_ids_to_remove = [901, 240, 881, 1467, 1472, 108] #contact authors\n",
        "\n",
        "df = df[~df[\"Paper ID (from shared sheet)\"].isin(paper_ids_to_remove)]\n",
        "df[df[\"Paper ID (from shared sheet)\"] == 108]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJamFvhNzDEU",
        "outputId": "96269eff-4406-4ec7-cbc6-98e1f372b4f6"
      },
      "outputs": [],
      "source": [
        "print(\"No of reviews after duplicate removal: \", len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m49r76Sr3EB-",
        "outputId": "15518db0-e158-4e6a-83d5-142b51e3cab0"
      },
      "outputs": [],
      "source": [
        "# Drop rows with all NaN or NaT values\n",
        "df = df.dropna(how=\"all\")\n",
        "\n",
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmSIfxId3_c2",
        "outputId": "6571985b-998d-4928-b495-4c525bf33db5"
      },
      "outputs": [],
      "source": [
        "# Remove papers with a year before 2014 - temporary messure. some incorrect paper year.\n",
        "\n",
        "# df[\"Paper Year\"] = df[\"Paper Year\"].astype(int)\n",
        "\n",
        "# # Check the updated column type\n",
        "# print(df[\"Paper Year\"].dtypes)\n",
        "\n",
        "df = df[df[\"Paper Year\"] >= 2014]\n",
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IN3bacOo9cfg",
        "outputId": "83a275c3-0e37-41cb-b079-c3b825fabad1"
      },
      "outputs": [],
      "source": [
        "print(\"Paper with wrong year: \", 1696-1665)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1M1RFW5P0q1R",
        "outputId": "2eba2204-7866-4ce1-905c-27b7f400402e"
      },
      "outputs": [],
      "source": [
        "count_yes = df[\"Paper Available\"].value_counts().get(\"Yes\", 0)\n",
        "print(count_yes)\n",
        "print(len(df)-count_yes + (len(Total_papers)-1702))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0PWp6MJ1jYJ",
        "outputId": "e2e366a0-7e26-4ac7-a8e0-ac81f433cc92"
      },
      "outputs": [],
      "source": [
        "count_yes = df[\"Paper in English\"].value_counts().get(\"Yes\", 0)\n",
        "print(count_yes)\n",
        "print(len(df)-count_yes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# No fo paper which are both not fully available and not in english \n",
        "overlap = df[(df[\"Paper Available\"] != \"Yes\") & (df[\"Paper in English\"] != \"Yes\")]\n",
        "len(overlap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZbAhJSve9-0",
        "outputId": "dca9d8d9-2623-4d44-de69-e094b6925639"
      },
      "outputs": [],
      "source": [
        "\n",
        "columns_to_analyze = [\"openml-suites-2021\", \"openml-python-2021\", \"openml-2014\", \"openml-r-2017\"]\n",
        "\n",
        "# Calculate the percentage of True values for each column\n",
        "percentages = Total_papers[columns_to_analyze].mean() * 100\n",
        "\n",
        "# Print the results\n",
        "for column, percentage in percentages.items():\n",
        "    print(f\"Percentage of papers with True for {column}: {percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "Xbi0A9kP628A",
        "outputId": "55539d2d-c75f-49c3-ca17-eea2ce740cf1"
      },
      "outputs": [],
      "source": [
        "# Filter rows not in final_df\n",
        "not_in_final_df = df[~((df[\"Paper Available\"] == \"Yes\") &  (df[\"Paper in English\"] == \"Yes\"))]\n",
        "\n",
        "print(len(not_in_final_df))\n",
        "# Display the rows not in final_df\n",
        "not_in_final_df[[\"Paper Available\", \"Paper in English\"]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhEJ8Azj9750",
        "outputId": "6b296d78-2d81-4512-d71e-2949647c7293"
      },
      "outputs": [],
      "source": [
        "# Should only consider paper available + in english\n",
        "available_papers = df[(df[\"Paper Available\"] == \"Yes\") & (df[\"Paper in English\"] == \"Yes\")]\n",
        "print(\"Final number of papers for analysis: \", len(available_papers))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Final statistics\n",
        "\n",
        "1. Total exctracted paper: 1719\n",
        "2. Paper used in analysis: 1482"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "7-9378NBGGRu",
        "outputId": "9dd41f98-b67f-42cf-c675-840750ae104c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Count the number of papers per year\n",
        "papers_per_year = available_papers[\"Paper Year\"].value_counts().sort_index()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(papers_per_year.index, papers_per_year.values, color=\"skyblue\", edgecolor=\"black\")\n",
        "\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Number of Papers\")\n",
        "plt.title(\"Number of OpenML Cited Papers Per Year\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(linestyle='--', alpha=0.2)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCdfjAQJYFR7",
        "outputId": "fe6bc34e-8aad-4787-a6b9-a03bda8c2ade"
      },
      "outputs": [],
      "source": [
        "# What percentage of the papers does just cite OpenML, but not actively interact with it?\n",
        "\n",
        "interaction_columns = [\n",
        "    \"Does the paper use datasets from OpenML?\",\n",
        "    \"Does the paper use a collection (at least 2 or more) of datasets that are defined by OpenML designated to do benchmarking (e.g., openml benchmarking suites)?\",\n",
        "    \"Does the paper use OpenML experiment data (i.e., utilise results from runs)? \",\n",
        "    \"Does the paper upload datasets to OpenML?\",\n",
        "    \"Does the paper upload experiment data to OpenML?\",\n",
        "    \"Does the paper interact in any other way with OpenML?\"\n",
        "]\n",
        "\n",
        "just_citing_papers = available_papers[(available_papers[interaction_columns] == \"No\").all(axis=1)]\n",
        "\n",
        "percentage_citing_only = (len(just_citing_papers) / len(available_papers)) * 100\n",
        "print(len(just_citing_papers), round(percentage_citing_only, 2))\n",
        "\n",
        "core_citing_papers = just_citing_papers[just_citing_papers[\"Does the paper have at least 1 current OpenML Core Member as co-author?\"] == \"Yes\"]\n",
        "print(len(core_citing_papers), round((len(core_citing_papers)/len(just_citing_papers))*100,2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdQJYPjIfZIW",
        "outputId": "ca320159-1bd6-44d0-ce76-2084a7c35b57"
      },
      "outputs": [],
      "source": [
        "# out of above papers which just cites openml, % of paper citing Openml-2014 paper\n",
        "\n",
        "just_citing_openml_2014 = just_citing_papers[just_citing_papers[\"openml-2014\"] == True]\n",
        "print(len(just_citing_openml_2014), (len(just_citing_openml_2014) / len(just_citing_papers)) * 100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "Oi5-nmvVHPVg",
        "outputId": "10825865-df06-402d-a955-e59355d8abcd"
      },
      "outputs": [],
      "source": [
        "english_counts = df[\"Paper in English\"].value_counts()\n",
        "\n",
        "plt.figure(figsize=(18, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.pie(english_counts, labels=english_counts.index, autopct=\"%1.1f%%\")\n",
        "plt.title(\"Papers in English\")\n",
        "\n",
        "\n",
        "core_member_counts = available_papers[\"Does the paper have at least 1 current OpenML Core Member as co-author?\"].value_counts()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pie(core_member_counts, labels=core_member_counts.index, autopct=\"%1.1f%%\")\n",
        "plt.title(\"Papers with OpenML Core Member Co-author\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "V3xB5ML_IMPu",
        "outputId": "e79cf3ed-8705-46aa-b125-aea636592a38"
      },
      "outputs": [],
      "source": [
        "# Datasets\n",
        "\n",
        "dataset_papers = available_papers[available_papers[\"Does the paper use datasets from OpenML?\"] == \"Yes\"]\n",
        "dataset_percentage = (len(dataset_papers) / len(available_papers)) * 100\n",
        "print(dataset_percentage, (len(dataset_papers)))\n",
        "print(\"Number of paper by core-authors: \",len(dataset_papers[dataset_papers[\"Does the paper have at least 1 current OpenML Core Member as co-author?\"] == \"Yes\"]))\n",
        "\n",
        "unclear_dataset_papers = available_papers.dropna(subset=[\"If unclear, one sentence explanation?\"])\n",
        "unclear_dataset_papers = unclear_dataset_papers[[\"Does the paper use datasets from OpenML?\", \"If unclear, one sentence explanation?\"]]\n",
        "unclear_dataset_papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4VmZNdkKJAC",
        "outputId": "ef9a9676-eaac-4315-a54d-4b3d924b7d4f"
      },
      "outputs": [],
      "source": [
        "# Benchmark\n",
        "benchmark_papers = available_papers[available_papers[\"Does the paper use a collection (at least 2 or more) of datasets that are defined by OpenML designated to do benchmarking (e.g., openml benchmarking suites)?\"] == \"Yes\"]\n",
        "benchmark_percentage = (len(benchmark_papers) / len(available_papers[available_papers[\"Paper Year\"] >= 2017])) * 100\n",
        "print(benchmark_percentage, len(benchmark_papers))\n",
        "\n",
        "benchmark_datasets = benchmark_papers[\"If yes, which benchmark suites?\"]\n",
        "\n",
        "print(\"Number of paper by core-authors: \",len(benchmark_papers[benchmark_papers[\"Does the paper have at least 1 current OpenML Core Member as co-author?\"] == \"Yes\"]))\n",
        "# Print list of benchmark datasets\n",
        "unique_benchmark_datasets = (\n",
        "    benchmark_datasets.str.lower().str.strip().unique()\n",
        ")\n",
        "\n",
        "\n",
        "unique_benchmark_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "RWw_HzTJKbs5",
        "outputId": "dc59bbd1-7f55-4cde-ac62-4f23bfcc3701"
      },
      "outputs": [],
      "source": [
        "#  Experiment data (Runs)\n",
        "\n",
        "experiment_papers = available_papers[available_papers[\"Does the paper use OpenML experiment data (i.e., utilise results from runs)? \"] == \"Yes\"]\n",
        "experiment_percentage = (len(experiment_papers) / len(available_papers)) * 100\n",
        "\n",
        "print(len(experiment_papers) ,experiment_percentage)\n",
        "print(\"Number of paper by core-authors: \",len(experiment_papers[experiment_papers[\"Does the paper have at least 1 current OpenML Core Member as co-author?\"] == \"Yes\"]))\n",
        "\n",
        "\n",
        "experiment_papers[[\"Paper ID (from shared sheet)\", \"if yes: short (e.g., 1 sentence) explanation: how does it use this data?\"]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        },
        "id": "SV6nngGINXF7",
        "outputId": "deb3fa55-a121-4c45-c701-048d16258775"
      },
      "outputs": [],
      "source": [
        "upload_datasets = available_papers[available_papers[\"Does the paper upload datasets to OpenML?\"] == \"Yes\"]\n",
        "\n",
        "upload_datasets_percentage = len(upload_datasets) / len(available_papers) * 100\n",
        "\n",
        "print( len(upload_datasets), upload_datasets_percentage)\n",
        "print(\"Number of paper by core-authors: \",len(upload_datasets[upload_datasets[\"Does the paper have at least 1 current OpenML Core Member as co-author?\"] == \"Yes\"]))\n",
        "\n",
        "\n",
        "upload_datasets[[\"Paper ID (from shared sheet)\", \"Does the paper upload datasets to OpenML?\", \"If yes, which dataset?\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 931
        },
        "id": "PCvmqWoROv6y",
        "outputId": "6d0255f0-44de-47a5-dc59-a6e9e3dd51e3"
      },
      "outputs": [],
      "source": [
        "upload_experiment_data_papers = available_papers[available_papers[\"Does the paper upload experiment data to OpenML?\"] == \"Yes\"]\n",
        "\n",
        "upload_experiment_data_percentage = len(upload_experiment_data_papers) / len(available_papers) * 100\n",
        "\n",
        "print(len(upload_experiment_data_papers),upload_experiment_data_percentage)\n",
        "print(\"Number of paper by core-authors: \",len(upload_experiment_data_papers[upload_experiment_data_papers[\"Does the paper have at least 1 current OpenML Core Member as co-author?\"] == \"Yes\"]))\n",
        "\n",
        "upload_experiment_data_papers[[\"Paper ID (from shared sheet)\", \"if yes: short (e.g., 1 sentence) explanation: what type of experiments?\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "fFj6rttJB0Bn",
        "outputId": "a2e1ada9-a7f1-4b8c-a46c-2b17250d842b"
      },
      "outputs": [],
      "source": [
        "# paper year not mentioned.\n",
        "available_papers[available_papers[\"Paper Year\"].isna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 940
        },
        "id": "i_a21wsUAeO4",
        "outputId": "3af31895-f2dd-4056-8d9b-65236d9ee74b"
      },
      "outputs": [],
      "source": [
        "\n",
        "available_papers[\"Paper Year\"] = available_papers[\"Paper Year\"].astype(int)\n",
        "\n",
        "available_papers[\"Datasets Used\"] = available_papers[\"Does the paper use datasets from OpenML?\"] == \"Yes\"\n",
        "available_papers[\"Benchmark Used\"] = available_papers[\"Does the paper use a collection (at least 2 or more) of datasets that are defined by OpenML designated to do benchmarking (e.g., openml benchmarking suites)?\"] == \"Yes\"\n",
        "\n",
        "# Group data by year for visualization\n",
        "datasets_by_year = available_papers.groupby(\"Paper Year\")[\"Datasets Used\"].sum()\n",
        "benchmarks_by_year = available_papers.groupby(\"Paper Year\")[\"Benchmark Used\"].sum()\n",
        "\n",
        "# Filter out the year 2025 from the data for plotting\n",
        "datasets_by_year_filtered = datasets_by_year[datasets_by_year.index != 2025]\n",
        "benchmarks_by_year_filtered = benchmarks_by_year[benchmarks_by_year.index != 2025]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "ax.plot(\n",
        "    datasets_by_year_filtered.index,\n",
        "    datasets_by_year_filtered.values,\n",
        "    color=\"blue\",\n",
        "    marker=\"o\",\n",
        "    label=\"Papers Using Openml Datasets'\"\n",
        ")\n",
        "\n",
        "# Add data labels for datasets\n",
        "for i, value in enumerate(datasets_by_year_filtered.values):\n",
        "    ax.text(datasets_by_year_filtered.index[i], value, str(value), color=\"blue\", fontsize=10, ha=\"right\", va=\"bottom\")\n",
        "\n",
        "# Line plot for benchmarks used\n",
        "ax.plot(\n",
        "    benchmarks_by_year_filtered.index,\n",
        "    benchmarks_by_year_filtered.values,\n",
        "    color=\"red\",\n",
        "    marker=\"o\",\n",
        "    label=\"Papers Using Openml Benchmarking Suites\"\n",
        ")\n",
        "\n",
        "# Add data labels for benchmarks\n",
        "for i, value in enumerate(benchmarks_by_year_filtered.values):\n",
        "    ax.text(benchmarks_by_year_filtered.index[i], value, str(value), color=\"red\", fontsize=10, ha=\"right\", va=\"bottom\")\n",
        "\n",
        "\n",
        "# # Add horizontal annotations on the x-axis\n",
        "# annotations = {\n",
        "#     2016: \"openml R\",\n",
        "#     2017: \"preprint of OpenML Benchmarking Suites paper\",\n",
        "#     2018: \"openml-python on PyPI\",\n",
        "#     2019: \"first paper for AutoML benchmark, openml R paper\",\n",
        "#     2021: \"NeurIPS paper on OpenML Benchmarking Suites, OpenML Python paper\"\n",
        "# }\n",
        "\n",
        "# import textwrap\n",
        "\n",
        "# # Add vertical lines and annotations\n",
        "# for year, annotation in annotations.items():\n",
        "#     # Split the annotation into multiple lines if it's too long\n",
        "#     wrapped_text = textwrap.fill(annotation, width=30)  # Adjust width based on your preference\n",
        "\n",
        "#     # Add the vertical line\n",
        "#     ax.vlines(x=year, ymin=0, ymax=max(100, 100) * 1.05, color='green', linestyle='--', lw=1)\n",
        "\n",
        "#     # Place the annotation text horizontally at the top of the vertical line\n",
        "#     ax.text(year, max(150, 150) * 1.05, wrapped_text, ha='center', va='top', fontsize=10, color='green', rotation=90)\n",
        "\n",
        "\n",
        "ax.set_xticks(range(min(datasets_by_year.index), max(datasets_by_year.index)))\n",
        "ax.set_xticklabels(range(min(datasets_by_year.index), max(datasets_by_year.index)), rotation=45)\n",
        "\n",
        "# Labels and title\n",
        "ax.set_xlabel(\"Year\")\n",
        "ax.set_ylabel(\"Number of Papers\")\n",
        "ax.set_title(\"Datasets and Benchmarks Used by Papers per Year\")\n",
        "ax.legend()\n",
        "\n",
        "ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "JzMytxQAPYdR",
        "outputId": "dbe3d546-c589-47d8-fc47-b5e073bbf28d"
      },
      "outputs": [],
      "source": [
        "other_interactions = available_papers[available_papers[\"Does the paper interact in any other way with OpenML?\"] == \"Yes\"]\n",
        "\n",
        "other_interactions_percentage = len(other_interactions) / len(available_papers) * 100\n",
        "\n",
        "print(len(other_interactions) , other_interactions_percentage)\n",
        "print(\"Number of paper by core-authors: \",len(other_interactions[other_interactions[\"Does the paper have at least 1 current OpenML Core Member as co-author?\"] == \"Yes\"]))\n",
        "\n",
        "\n",
        "other_interactions[[\"Does the paper interact in any other way with OpenML?\", \"if yes: short (e.g., 1 sentence) explanation how?\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 757
        },
        "id": "GLFvS-TJRLpW",
        "outputId": "dc9d250b-809f-4c26-d0d1-2b87fb403deb"
      },
      "outputs": [],
      "source": [
        "starred_papers = available_papers[available_papers[\"Star it as some cool project to be showcased in our paper?\"] == \"Yes\"]\n",
        "\n",
        "print(len(starred_papers))\n",
        "\n",
        "starred_papers[[\"Paper ID (from shared sheet)\", \"If yes, please motivate your answer\"]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "VPGMHf91yR4h"
      },
      "outputs": [],
      "source": [
        "# output_file = \"starred_papers.csv\"\n",
        "# starred_papers.to_csv(output_file, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuPbcG5FRk0U",
        "outputId": "e067c337-ff54-4456-d3ec-8ce1f4a8cb13"
      },
      "outputs": [],
      "source": [
        "thesis_papers = available_papers[available_papers[\"Is the paper a thesis (Bachelor's, Master's, or PhD)?\"] == \"Yes\"]\n",
        "\n",
        "# Also, check for any occurrence of the word 'thesis' in the \"Optional short description\"\n",
        "thesis_keyword_papers = available_papers[available_papers[\"Optional short description\"].str.contains(\"thesis\", case=False, na=False)]\n",
        "all_thesis_papers = pd.concat([thesis_papers, thesis_keyword_papers]).drop_duplicates()\n",
        "\n",
        "\n",
        "thesis_percentage = len(all_thesis_papers) / len(available_papers) * 100\n",
        "print(\"Number of paper by core-authors: \",len(all_thesis_papers[all_thesis_papers[\"Does the paper have at least 1 current OpenML Core Member as co-author?\"] == \"Yes\"]))\n",
        "\n",
        "print(len(all_thesis_papers), thesis_percentage)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "cEW3aN-mS-WO"
      },
      "outputs": [],
      "source": [
        "# Differentiate in skill level between core members and non-core members\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
