{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "\n",
    "json_dir_path = \"../scraped_paper_reviewer_assignment\"  # Directory containing the JSON files\n",
    "csv_file_path = \"merged.csv\"  # Path to the input CSV file\n",
    "output_csv_file_path = \"updated_merged.csv\"  # Path to the output CSV file\n",
    "log_file_path = \"unmatched_papers.log\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load all JSON data from the directory\n",
    "title_to_url = {}\n",
    "for json_file_name in os.listdir(json_dir_path):\n",
    "    if json_file_name.endswith(\".json\"):  # Only process JSON files\n",
    "        json_file_path = os.path.join(json_dir_path, json_file_name)\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "            try:\n",
    "                papers_data = json.load(json_file)\n",
    "                for paper in papers_data:\n",
    "                    # Check for either \"paper_url\" or \"link\" field\n",
    "                    url = paper.get(\"paper_url\") or paper.get(\"link\")\n",
    "                    if url:\n",
    "                        title_to_url[paper[\"title\"]] = url\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error decoding {json_file_name}, skipping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total collected papers:  71\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Croissant: A metadata format for ml-ready datasets': 'https://proceedings.neurips.cc/paper_files/paper/2024/hash/9547b09b722f2948ff3ddb5d86002bc0-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       " 'Tunetables: Context optimization for scalable prior-data fitted networks': 'https://proceedings.neurips.cc/paper_files/paper/2024/hash/97dc07f1253ab33ee514f395a82fa7cc-Abstract-Conference.html',\n",
       " 'Genetic programming for feature selection based on feature removal impact in high-dimensional symbolic regression': 'https://ieeexplore.ieee.org/abstract/document/10466603/',\n",
       " 'Dissecting sample hardness: A fine-grained analysis of hardness characterization methods for data-centric ai': 'https://arxiv.org/abs/2403.04551',\n",
       " 'Deep learning through a telescoping lens: A simple model provides empirical insights on grokking, gradient boosting &amp; beyond': 'https://proceedings.neurips.cc/paper_files/paper/2024/hash/df334022279996b07e0870a629c18857-Abstract-Conference.html',\n",
       " 'MLSea: a semantic layer for discoverable machine learning': 'https://link.springer.com/chapter/10.1007/978-3-031-60635-9_11',\n",
       " 'Reshuffling resampling splits can improve generalization of hyperparameter optimization': 'https://proceedings.neurips.cc/paper_files/paper/2024/hash/47811ee68103bfcde7ca2223fccefb3a-Abstract-Conference.html',\n",
       " 'Embedded ethics for responsible artificial intelligence systems (EE-RAIS) in disaster management: A conceptual model and its deployment': 'https://link.springer.com/article/10.1007/s43681-023-00309-1',\n",
       " 'Designing an ml auditing criteria catalog as starting point for the development of a framework': 'https://ieeexplore.ieee.org/abstract/document/10464309/',\n",
       " \"Hyperparameter tuning MLP's for probabilistic time series forecasting\": 'https://link.springer.com/chapter/10.1007/978-981-97-2266-2_21',\n",
       " 'ROPAC: rule optimized aggregation classifier': 'https://www.sciencedirect.com/science/article/pii/S0957417424007632',\n",
       " 'Binning as a pretext task: Improving self-supervised learning in tabular domains': 'https://arxiv.org/abs/2405.07414',\n",
       " '4dbinfer: A 4d benchmarking toolbox for graph-centric predictive modeling on relational dbs': 'https://arxiv.org/abs/2404.18209',\n",
       " 'AutoMLBench: a comprehensive experimental evaluation of automated machine learning frameworks': 'https://www.sciencedirect.com/science/article/pii/S0957417423033791',\n",
       " 'Sok: A review of differentially private linear models for high-dimensional data': 'https://ieeexplore.ieee.org/abstract/document/10516654/',\n",
       " 'Multiclass graph-based large margin classifiers: Unified approach for support vectors and neural networks': 'https://ieeexplore.ieee.org/abstract/document/10616182/',\n",
       " '4DBInfer: A 4d benchmarking toolbox for graph-centric predictive modeling on RDBs': 'https://proceedings.neurips.cc/paper_files/paper/2024/hash/2fd67447702c8eff5683dda507a1b0a2-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       " 'Tabular transfer learning via prompting llms': 'https://arxiv.org/abs/2408.11063',\n",
       " 'T-Explainer: A model-agnostic explainability framework based on gradients': 'https://arxiv.org/abs/2404.16495',\n",
       " 'Learning to transfer for evolutionary multitasking': 'https://arxiv.org/abs/2406.14359',\n",
       " 'CCO: A cluster core-based oversampling technique for improved class-imbalanced learning': 'https://ieeexplore.ieee.org/abstract/document/10555431/',\n",
       " 'Querying easily flip-flopped samples for deep active learning': 'https://arxiv.org/abs/2401.09787',\n",
       " 'Data science with llms and interpretable models': 'https://arxiv.org/abs/2402.14474',\n",
       " 'Comparative evaluation of imbalanced data management techniques for solving classification problems on imbalanced datasets': 'http://www.iapress.org/index.php/soic/article/view/1890',\n",
       " 'Sequential large language model-based hyper-parameter optimization': 'https://arxiv.org/abs/2410.20302',\n",
       " 'Metaqure: Meta-learning from model quality and resource consumption': 'https://link.springer.com/chapter/10.1007/978-3-031-70368-3_13',\n",
       " 'Enhancing cross-modal fine-tuning with gradually intermediate modality generation': 'https://arxiv.org/abs/2406.09003',\n",
       " 'InterpreTabNet: distilling predictive signals from tabular data by salient feature interpretation': 'https://arxiv.org/abs/2406.00426',\n",
       " 'AutoML insights: Gaining confidence to Operationalize Predictive models': 'https://www.intechopen.com/online-first/1179126',\n",
       " 'Optimal weighted random forests': 'http://www.jmlr.org/papers/v25/23-0607.html',\n",
       " 'Mldm: Machine learning data market based on multi-agent systems': 'https://ieeexplore.ieee.org/abstract/document/10528655/',\n",
       " 'Lf-transformer: Latent factorizer transformer for tabular learning': 'https://ieeexplore.ieee.org/abstract/document/10401112/',\n",
       " 'A Hybrid approach for binary classification of imbalanced data': 'https://www.worldscientific.com/doi/abs/10.1142/S1469026824500135',\n",
       " 'Entropy-Regularized Token-Level Policy Optimization for Language Agent Reinforcement': 'https://arxiv.org/abs/2402.06700',\n",
       " 'Shapley-based data valuation method for the machine learning data markets (MLDM)': 'https://link.springer.com/chapter/10.1007/978-3-031-62700-2_16',\n",
       " 'Uncertainty estimation with recursive feature machines': 'https://openreview.net/forum?id=TBKLXswKnO',\n",
       " 'Kglids: A platform for semantic abstraction, linking, and automation of data science': 'https://ieeexplore.ieee.org/abstract/document/10597753/',\n",
       " 'Distributed genetic algorithm for feature selection': 'https://arxiv.org/abs/2401.10846',\n",
       " 'Fine-tuned In-Context Learning Transformers are Excellent Tabular Data Classifiers': 'https://arxiv.org/abs/2405.13396',\n",
       " 'Information leakage detection through approximate bayes-optimal prediction': 'https://arxiv.org/abs/2401.14283',\n",
       " 'Quantifying local model validity using active learning': 'https://arxiv.org/abs/2406.07474',\n",
       " 'MoRE-LLM: Mixture of Rule Experts Guided by a Large Language Model': 'https://ieeexplore.ieee.org/abstract/document/10884197/',\n",
       " 'Generalizing Neural Additive Models via Statistical Multimodal Analysis': 'https://openreview.net/forum?id=xLg8ljlEba',\n",
       " 'Advancing research reproducibility in machine learning through blockchain technology': 'https://content.iospress.com/articles/informatica/infor553',\n",
       " 'Learning a decision tree algorithm with transformers': 'https://arxiv.org/abs/2402.03774',\n",
       " 'Trust regions for explanations via black-box probabilistic certification': 'https://arxiv.org/abs/2402.11168',\n",
       " 'Iterative regularization with k-support norm: an important complement to sparse recovery': 'https://ojs.aaai.org/index.php/AAAI/article/view/29057',\n",
       " 'Tabflex: Scaling tabular learning to millions with linear attention': 'https://openreview.net/forum?id=f8aganC0tN',\n",
       " 'Empirical Comparison between Cross-Validation and Mutation-Validation in Model Selection': 'https://link.springer.com/chapter/10.1007/978-3-031-58553-1_5',\n",
       " 'Cyber diversity index for sustainable self-control of machines': 'https://www.tandfonline.com/doi/abs/10.1080/01969722.2022.2081896',\n",
       " 'Forward composition propagation for explainable neural reasoning': 'https://ieeexplore.ieee.org/abstract/document/10384511/',\n",
       " 'Learning No-Regret Sparse Generalized Linear Models with Varying Observation (s)': 'https://openreview.net/forum?id=wISvONp3Kq',\n",
       " 'Advancing deep active learning &amp; data subset selection: Unifying principles with information-theory intuitions': 'https://arxiv.org/abs/2401.04305',\n",
       " 'Comparative Analysis of Oversampling Techniques and Deep Learning for Imbalanced Tabular Data': 'https://ieeexplore.ieee.org/abstract/document/10903102/',\n",
       " 'New Insight of Variance reduce in Zero-Order Hard-Thresholding: Mitigating Gradient Error and Expansivity Contradictions': 'https://openreview.net/forum?id=fjf3YenThE',\n",
       " 'Fast and Understandable Nonlinear Supervised Dimensionality Reduction': 'https://link.springer.com/chapter/10.1007/978-3-031-78977-9_25',\n",
       " 'LLM Prompting Strategies in Automated Machine Learning': 'https://ieeexplore.ieee.org/abstract/document/10908759/',\n",
       " 'Ein formales Modell fur Anwendungen Kunstlicher Intelligenz in Automatisierungssystemen': 'https://epub.sub.uni-hamburg.de/epub/volltexte/2025/183719/pdf/openHSU_16768.pdf#page=124',\n",
       " 'Decision Tree Induction via Semantically-Aware Evolution': 'https://openreview.net/forum?id=UyhRtB4hjN',\n",
       " 'Reliable statistical inference: controlling the false discovery proportion in high-dimensional multivariate estimators': 'https://theses.hal.science/tel-04935172/',\n",
       " 'Optimal Learning of Kernel Logistic Regression for Complex Classification Scenarios': 'https://openreview.net/forum?id=WlhVRh2rQ0',\n",
       " 'Towards Trustworwhy Machine Learning using Unlabeled Data': 'https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/698135/thesis.pdf?sequence=6',\n",
       " 'Constraint-aware machine learning': 'https://search.proquest.com/openview/8ea1111d565ecf43742237b6b55664ce/1?pq-origsite=gscholar&cbl=2026366&diss=y',\n",
       " 'Data Centric Optimisation in AutoML': 'https://estudogeral.uc.pt/retrieve/277691/MSc_Thesis_Joana_Simoes_Data_centric_optimisation_in_AutoML.pdf',\n",
       " 'Multi-Agent-Based Collaborative Machine Learning in Distributed Resource Environments': 'https://hammer.purdue.edu/articles/thesis/Multi-Agent-Based_Collaborative_Machine_Learning_in_Distributed_Resource_Environments/26314504',\n",
       " 'Real-time human activity recognition with KANs': 'https://repositorio.ipl.pt/entities/publication/72ac1b36-4ac2-4429-a851-54e09fbaba3f',\n",
       " 'Range-limited Augmentation for Few-shot Learning in Tabular Data': 'https://openreview.net/forum?id=kTH3bEH6hW',\n",
       " 'Optimized Oversampling': 'https://openreview.net/forum?id=ZSzmWtY31e',\n",
       " 'Distribution Shift Aware Neural Feature Transformation': 'https://openreview.net/forum?id=TXjYOslkUh',\n",
       " 'An opensource library for AutoML multimodal clustering on Apache Spark': 'https://www.mathnet.ru/eng/znsl7550',\n",
       " \"Amélioration des explications attributives locales pour appuyer l'analyse prédictive par apprentissage automatique: application au secteur de la santé et aux outils d'&nbsp;…\": 'https://publications.ut-capitole.fr/id/eprint/50553/'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Total collected papers: \", len(title_to_url))\n",
    "title_to_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 2:\n",
    "Processes existing CSV file, updates each row by adding a \"Paper URL\" column, \n",
    "and tracks papers that do not have a matching URL. \n",
    "\"\"\"\n",
    "\n",
    "updated_rows = []\n",
    "unmatched_papers = []\n",
    "\n",
    "with open(csv_file_path, 'r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    header = next(csv_reader)  # Extract the header\n",
    "    if \"Paper URL\" not in header:\n",
    "        header.append(\"Paper URL\")  # Add a new column for URLs if not present\n",
    "\n",
    "    for row in csv_reader:\n",
    "        paper_title = row[1].strip()  # Assuming the first column contains paper titles\n",
    "        \n",
    "        if paper_title in title_to_url:\n",
    "            row.append(title_to_url[paper_title])  # Add the URL to the row\n",
    "        else:\n",
    "            row.append(\"\")  # Add an empty URL if not found\n",
    "            unmatched_papers.append(paper_title)  # Log the unmatched paper\n",
    "        updated_rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 : Write the updated rows to the output CSV file\n",
    "with open(output_csv_file_path, 'w', encoding='utf-8', newline='') as output_csv:\n",
    "    csv_writer = csv.writer(output_csv)\n",
    "    csv_writer.writerow(header)  # Write the header\n",
    "    csv_writer.writerows(updated_rows)  # Write the updated rows\n",
    "\n",
    "# Step 4: Write the unmatched papers to a log file\n",
    "with open(log_file_path, 'w', encoding='utf-8') as log_file:\n",
    "    log_file.write(\"Unmatched Papers:\\n\")\n",
    "    for paper in unmatched_papers:\n",
    "        log_file.write(f\"{paper}\\n\")\n",
    "\n",
    "print(f\"Processing complete. Updated CSV saved to {output_csv_file_path}, unmatched papers logged to {log_file_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assignments complete. Papers with reviewers saved to 'updated_merged_reviewers.csv', and unassigned papers saved to 'unassigned_papers.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Assign Reviewers\n",
    "import csv\n",
    "\n",
    "# List of reviewers\n",
    "reviewers = [] #Names removed for anonymity \n",
    "\n",
    "# File paths\n",
    "input_csv_file = \"updated_merged.csv\"  # Input CSV file with 1719 papers\n",
    "output_csv_file = \"updated_merged_reviewers.csv\"  # Output CSV file with assignments\n",
    "unassigned_csv_file = \"unassigned_papers.csv\"  # Output CSV file for unassigned papers\n",
    "\n",
    "# Read the list of papers\n",
    "with open(input_csv_file, 'r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    header = next(csv_reader)\n",
    "    papers = [row for row in csv_reader]\n",
    "\n",
    "# Assign papers to reviewers\n",
    "reviewer_assignments = {reviewer: [] for reviewer in reviewers}\n",
    "unassigned_papers = []\n",
    "\n",
    "# Total papers per reviewer\n",
    "papers_per_reviewer = 100\n",
    "total_reviewers = len(reviewers)\n",
    "total_papers = len(papers)\n",
    "\n",
    "# Assign papers to reviewers, ensuring all papers are accounted for\n",
    "for idx, paper in enumerate(papers):\n",
    "    if idx < total_reviewers * papers_per_reviewer:\n",
    "        # Assign to a reviewer in a round-robin fashion\n",
    "        reviewer = reviewers[idx // papers_per_reviewer]\n",
    "        reviewer_assignments[reviewer].append(paper + [reviewer])  # Add reviewer to the paper\n",
    "    else:\n",
    "        unassigned_papers.append(paper)  # Papers left unassigned\n",
    "\n",
    "# Write assignments to the output CSV file, including all papers\n",
    "with open(output_csv_file, 'w', encoding='utf-8', newline='') as output_csv:\n",
    "    csv_writer = csv.writer(output_csv)\n",
    "    csv_writer.writerow(header + [\"Reviewer\"])  # Add reviewer column to the header\n",
    "    for reviewer, papers in reviewer_assignments.items():\n",
    "        csv_writer.writerows(papers)\n",
    "\n",
    "    # Add the unassigned papers to the output CSV (without reviewers)\n",
    "    for paper in unassigned_papers:\n",
    "        csv_writer.writerow(paper + [''])  # Add empty reviewer field for unassigned papers\n",
    "\n",
    "# Write unassigned papers to a separate CSV file (optional)\n",
    "with open(unassigned_csv_file, 'w', encoding='utf-8', newline='') as unassigned_csv:\n",
    "    csv_writer = csv.writer(unassigned_csv)\n",
    "    csv_writer.writerow(header)  # Use the original header\n",
    "    csv_writer.writerows(unassigned_papers)\n",
    "\n",
    "print(f\"Assignments complete. Papers with reviewers saved to '{output_csv_file}', and unassigned papers saved to '{unassigned_csv_file}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Step:\n",
    "\n",
    "Additional 71 papers were collected in March 2025. These papers were accepted in 2024, but were not yet published when the data was collected the first time. \n",
    "The collected papers cite Openml-2014 paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1720\n",
      "Updated ../data/collected_papers.csv successfully. Added 67 new papers.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Addition of the Last 71 papers, which are extracted in March 2025 \n",
    "Contains papers accepted in 2024, but published afterwards in early 2025.\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import os\n",
    "\n",
    "collected_data_csv = \"../data/collected_papers.csv\"\n",
    "\n",
    "# Read existing CSV and find max Paper ID\n",
    "existing_titles = set()\n",
    "updated_rows = []\n",
    "\n",
    "if os.path.exists(collected_data_csv):\n",
    "    with open(collected_data_csv, 'r', encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        header = next(csv_reader)\n",
    "        updated_rows.append(header)\n",
    "\n",
    "        paper_id_index = header.index(\"Paper ID\")\n",
    "        title_index = header.index(\"Title\")\n",
    "        max_paper_id = 0\n",
    "\n",
    "        for row in csv_reader:\n",
    "            updated_rows.append(row)\n",
    "            existing_titles.add(row[title_index].strip())\n",
    "            max_paper_id = max(max_paper_id, int(row[paper_id_index]))  # Track max Paper ID\n",
    "\n",
    "else:\n",
    "    print(f\"{collected_data_csv} not found. Creating a new file.\")\n",
    "    header = [\"Status\", \"Paper ID\", \"Title\", \"openml-suites-2021\", \"openml-python-2021\",\n",
    "              \"openml-2014\", \"openml-r-2017\", \"paper URL\"]\n",
    "    updated_rows.append(header)\n",
    "    max_paper_id = 0\n",
    "\n",
    "print(max_paper_id)\n",
    "# Add missing papers from title_to_url\n",
    "new_paper_id = max_paper_id + 1\n",
    "for title, url in title_to_url.items():\n",
    "    if title not in existing_titles:\n",
    "        updated_rows.append([\"FALSE\", new_paper_id, title, \"FALSE\", \"FALSE\", \"TRUE\", \"FALSE\", url])\n",
    "        new_paper_id += 1\n",
    "\n",
    "# Write the updated data back to collected_data.csv\n",
    "with open(collected_data_csv, 'w', encoding='utf-8', newline='') as csv_file:\n",
    "    csv.writer(csv_file).writerows(updated_rows)\n",
    "\n",
    "print(f\"Updated {collected_data_csv} successfully. Added {new_paper_id - max_paper_id - 1} new papers.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
